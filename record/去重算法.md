# 去重算法

> 以下内容均整理自网络，并非原创，侵删。

## 问题描述

对海量数据判断某个数据是否存在？或者描述为对海量数据进行去重。

对于那些不是很大的数据量，HashSet 便可以以O(1)的时间复杂度解决，或者运用对哈希取模的方法实现存在海量数据的情况。但这里主要介绍主要的两种方法。

当然，主要的解法有 Bitmap 和布隆过滤器，并且这里我们也额外介绍一种用来去重文本的方法：SimHash算法。


## SimHash

SimHash 算法是 Google 公司进行海量网页去重的高效算法，它通过将原始的文本映射为64位的二进制数字串，然后通过比较二进制数字串的差异进而来表示原始文本内容的差异。

假设我们有海量的文本数据，我们需要根据文本内容将它们进行去重。而局部敏感 hash 算法可以将原始的文本内容映射为数字，即 hash 签名，而且较为相近的文本内容对应的 hash 签名也比较相近。

通过该方法可以达到对文本数据去重的目的。


## Bitmap

Bitmap 就是用一个 bit 位来标记某个元素对应的 value 的存在，而 key 即是这个元素。由于采用 bit 为单位来存储数据，因此在可以大大的节省空间开销。

这里的海量数据以整数为例，整数为4字节，也就是32bit，假设数据量N（N为最大值）= 100000000，由于 bitmap 是用一个 bit 来标识元素的存在，那么我们只需⌈N/8⌉个字节就可以把数据表示出来也就是12500000字节，约为13MB，大大减少了存储开销。

假设该数字为 num，则它所对应的 bit 索引为：第 ⌊num/32⌋ 个 bitmap 整数中的第 num%32 位。

通过每个数字的映射，我们可以得到所需要的 bitmap 整数个数，我们只需对每个 bitmap 整数的每一位进行遍历，当某位为1时，就证明该位所对应的整数存在于原始数据中，当我们从最低位向最高位遍历时，通过整数与 bit 索引的逆过程就可以的到原整数，将其一个个输出，我们就能得到原始数据的从小到大排序后的结果。

因此，我们可以通过某个整数出现的次数来判断其是否重复。


## 布隆过滤器

布隆过滤器（Bloom filter）是一个高空间利用率的概率性数据结构，被用于测试一个元素是否在集合中（由于集合无重复元素的性质，可用来判重）。

一个空的布隆过滤器是一串被置为0的 bit 数组（假设由m位）。同时，应该声明k个不同的散列函数生成一个统一随机分布，每一个散列函数都将元素映射到m个 bit 中的一个（k是一个小于m的常数，与加入过滤器中的元素个数成比例）。k与相应的m的选择由误判率决定。向过滤器中添加元素时，通过k个散列函数得到该元素对应的k个位置，并将这些位置置为1。查询某个元素或者测试是否与已有元素重复时，仍然可以通过k个散列函数得到对应的k个位置，判断这些位置是否为1（若全为1则在集合内，即重复）。




## Ref

- [https://www.jianshu.com/p/5448f130b94d](https://www.jianshu.com/p/5448f130b94d)
- [https://blog.csdn.net/tick_tock97/article/details/78619938](https://blog.csdn.net/tick_tock97/article/details/78619938)
- [https://blog.csdn.net/tick_tock97/article/details/78688159](https://blog.csdn.net/tick_tock97/article/details/78688159)
- [http://www.cnblogs.com/maybe2030/p/5203186.html](http://www.cnblogs.com/maybe2030/p/5203186.html)


